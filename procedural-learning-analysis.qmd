---
title: "procedural-learning-graded-entropy-data-analysis"
author: "Cherrie Chang"
format: html
editor: visual
---

# Data Analysis for Procedural Learning Graded Entropy Design Experiment

## Libraries
```{r}
#| label: Load R libraries
#| echo: false
#| message: false

library(here)
library(osfr)
library(dplyr)
library(tidyr)
library(tidyverse)
library(zoo)
library(jsonlite)
library(clipr)
library(reticulate)
library(lmerTest) # show p value in lmer

np <- import("numpy")
```

## Participants
```{r}
#| label: Demographic data
#| include: false
#| echo: false

df.demographics <- list.files(here("demographic-data/"), pattern = "*.csv", full.names=TRUE) %>%
  lapply(function(f) {
    matrix_size <- gsub('.*_(\\dx\\d)\\.csv', '\\1', basename(f))
    read.csv(f) %>%
      mutate(Matrix.size = matrix_size)
  }) %>%
  bind_rows() %>%
  filter(Status=="APPROVED")

overall_N <- nrow(df.demographics)
overall_mean_age <- mean(as.numeric(df.demographics$Age), na.rm=TRUE)
overall_median_age <- median(as.numeric(df.demographics$Age), na.rm=TRUE)
overall_sd_age <- sd(as.numeric(df.demographics$Age), na.rm=TRUE)
overall_age_range <- range(as.numeric(df.demographics$Age), na.rm=TRUE)

N_nationalities <- length(unique(df.demographics$Nationality))
N_languages <- length(unique(df.demographics$Language))

# Gender distribution
gender_counts <- table(df.demographics$Sex)
n_female <- gender_counts["Female"]
n_male <- gender_counts["Male"]
n_other <- sum(gender_counts[!names(gender_counts) %in% c("Female", "Male")], na.rm=TRUE)

df.demographics.group_summary <- df.demographics %>%
  group_by(Matrix.size) %>%
  summarise(N = n(),
            mean_age = mean(as.numeric(Age), na.rm=TRUE),
            sd_age = sd(as.numeric(Age), na.rm=TRUE),
            min_age = min(as.numeric(Age), na.rm=TRUE),
            max_age = max(as.numeric(Age), na.rm=TRUE))

group_N <- first(df.demographics.group_summary$N)
N_5x5 <- df.demographics.group_summary %>%
  filter(Matrix.size=="5x5") %>%
  pull(N)
```

## Experiment Configs
```{r}
#| label: Experiment configs that stay constant across participants
#| echo: false
#| include: false

EXPERIMENT_CONFIG <- list(
  osf_id_4x4 = "Emfhw",
  osf_id_5x5 = "Fraxt",
  osf_id_6x6 = "jx48y",
  osf_id_7x7 = "92jq7",
  osf_id_8x8 = "Tx3h7",
  key_mapping_4pos = c("d","f","j","k"),
  key_mapping_5pos = c("s","d","f","j","k"),
  key_mapping_6pos = c("s","d","f","j","k","l"),
  key_mapping_7pos = c("a","s","d","f","j","k","l"),
  key_mapping_8pos = c("a","s","d","f","j","k","l",";"),
  n_blocks = 20,
  rsi = 120,
  error_feedback_duration = 200,
  error_tone_duration = 100,
  correct_feedback_duration = 200,
  estimated_trial_duration = 500,
  accuracy_threshold = 0.65,
	rt_threshold = 1000
)
```

## Transition Matrices and Positional Entropies
```{r}
#| label: Original transition matrices
#| echo: false
#| include: false

# Shannon entropy (bits) of a single probability vector (skip zeros)
row_entropy <- function(prob_vec) {
  p <- prob_vec[prob_vec > 0]
  -sum(p * log2(p))
}

# Calculate positional entropy for each row in a transition matrix
positional_entropy <- function(mat) {
  apply(mat, MARGIN=1, row_entropy)
}

transition_matrix_4x4 <- matrix(unlist(np$load(here("assets/transition-matrices/matrix_4x4.npy"))$tolist()), nrow=4, byrow=TRUE)
transition_matrix_5x5 <- matrix(unlist(np$load(here("assets/transition-matrices/matrix_5x5.npy"))$tolist()), nrow=5, byrow=TRUE)
transition_matrix_6x6 <- matrix(unlist(np$load(here("assets/transition-matrices/matrix_6x6.npy"))$tolist()), nrow=6, byrow=TRUE)
transition_matrix_7x7 <- matrix(unlist(np$load(here("assets/transition-matrices/matrix_7x7.npy"))$tolist()), nrow=7, byrow=TRUE)
transition_matrix_8x8 <- matrix(unlist(np$load(here("assets/transition-matrices/matrix_8x8.npy"))$tolist()), nrow=8, byrow=TRUE)
all_original_matrices <- list(
  "4x4" = transition_matrix_4x4,
  "5x5" = transition_matrix_5x5,
  "6x6" = transition_matrix_6x6,
  "7x7" = transition_matrix_7x7,
  "8x8" = transition_matrix_8x8
)

all_positional_entropies <- lapply(all_original_matrices, positional_entropy)
all_positional_entropies_df <- lapply(names(all_positional_entropies), function(size) {
  data.frame(
    Matrix.size = size,
    Position = 1:length(all_positional_entropies[[size]]),
    Entropy = all_positional_entropies[[size]]
  )
}) %>%
  bind_rows()

pos_1_entropy <- all_positional_entropies_df %>%
  group_by(Matrix.size) %>%
  filter(Position==1) %>%
  ungroup() %>%
  summarize(range=range(Entropy), mean=mean(Entropy), sd=sd(Entropy))

pos_n_entropy <- all_positional_entropies_df %>%
  group_by(Matrix.size) %>%
  filter(Position==max(Position)) %>%
  ungroup() %>%
  summarize(range=range(Entropy), mean=mean(Entropy), sd=sd(Entropy))

entropy_gradient_summary <- all_positional_entropies_df %>%
  group_by(Matrix.size) %>%
  summarize(entropy_diffs = list(diff(Entropy))) %>%
  ungroup() %>%
  summarize(
    range_diff = list(range(unlist(entropy_diffs))),
    mean_diff = mean(unlist(entropy_diffs)),
    sd_diff = sd(unlist(entropy_diffs))
  )
```

## Data download
```{r}
#| label: Retrieve data from OSF
#| echo: false
#| include: false
#| cache: true

files_4x4 <- osf_retrieve_node(EXPERIMENT_CONFIG$osf_id_4x4) %>%
  osf_ls_files(n_max = Inf) %>%
  osf_download(path = here("data/4x4-data"), conflicts="skip")

files_5x5 <- osf_retrieve_node(EXPERIMENT_CONFIG$osf_id_5x5) %>%
  osf_ls_files(n_max = Inf) %>%
  osf_download(path = here("data/5x5-data"), conflicts="skip")

files_6x6 <- osf_retrieve_node(EXPERIMENT_CONFIG$osf_id_6x6) %>%
  osf_ls_files(n_max = Inf) %>%
  osf_download(path = here("data/6x6-data"), conflicts="skip")

files_7x7 <- osf_retrieve_node(EXPERIMENT_CONFIG$osf_id_7x7) %>%
  osf_ls_files(n_max = Inf) %>%
  osf_download(path = here("data/7x7-data"), conflicts="skip")

files_8x8 <- osf_retrieve_node(EXPERIMENT_CONFIG$osf_id_8x8) %>%
  osf_ls_files(n_max = Inf) %>%
  osf_download(path = here("data/8x8-data"), conflicts="skip")

# But there is some bug in osfr that causes one file name to be duplicated and one file name to be skipped, so alternatively just download manually from OSF and put in data/ folder:
files_4x4 <- list.files("data/4x4-data")
files_5x5 <- list.files("data/5x5-data")
files_6x6 <- list.files("data/6x6-data")
files_7x7 <- list.files("data/7x7-data")
files_8x8 <- list.files("data/8x8-data")
```
```{r}
#| label: Bind all data together
#| echo: false
#| include: false
#| cache: true
df.raw <- list.files(here("data/"), pattern = "*.csv", full.names=TRUE, recursive=TRUE) %>%
  .[!grepl("pilot-data", .)] %>%
  lapply(read.csv) %>%
  bind_rows()
```

## Data cleanup
```{r}
#| label: Filter out unnecessary columns
#| echo: false
#| include: false
#| cache: true
df <- subset(df.raw, select=c(trial_index, subject_id, matrix_size, trials_per_block, total_trials, practice_trials, transition_matrix, conditional_entropies, phase, block, experiment_trial_type, trial_in_block, overall_trial, position, correct_key, response, correct, rt, conditional_entropy, surprisal, questionnaire_item))
```

```{r}
#| label: Clean up data formatting
#| echo: false
#| include: false

# Convert NA values
df <- df %>%
  mutate(across(where(is.character), ~{
    x <- .
    x[x %in% c("NA", "na", "null", "NULL", "")] <- NA
    x
  }))

# Convert boolean values
df <- df %>%
  mutate(across(where(is.character), ~ {
    x <- tolower(.)
    if (all(x %in% c("true", "false", NA))) {
      as.logical(x)
    } else {
      .
    }
  }))

# Convert numeric values
df <- df %>%
  mutate(across(where(is.character), ~ {
    x <- .
    is_num <- suppressWarnings(!is.na(as.numeric(x)) | is.na(x))
    if (all(is_num)) as.numeric(x) else x
  }))

df <- df %>%
  mutate(conditional_entropies = map(conditional_entropies, fromJSON)) %>%
  mutate(transition_matrix = map(transition_matrix, fromJSON))
```
```{r}
#| label: Fill down overall_trial, trial_in_block and block
#| echo: false
#| include: false
#| cache: true
df <- df %>%
 group_by(subject_id) %>%
    arrange(subject_id, row_number()) %>%
    mutate(
      overall_trial = if_else(
        experiment_trial_type %in% c("feedback", "rsi"),
        zoo::na.locf(overall_trial, na.rm = FALSE),  # Fill down
        overall_trial  # Keep as-is
      ),
      trial_in_block = if_else(
        experiment_trial_type %in% c("feedback", "rsi"),
        zoo::na.locf(trial_in_block, na.rm = FALSE), 
        trial_in_block
      ),
      block = if_else(
        experiment_trial_type %in% c("feedback", "rsi"),
        zoo::na.locf(block, na.rm = FALSE), 
        block
      )
    ) %>%
    ungroup()
```
```{r}
#| label: Sanity check for unique subject_ids, matrix size, total number of trials, practice trials, transition matrices, conditional entropies and sequences across subjects and conditions
#| echo: false
#| include: false
#| cache: true

unique_subjects <- unique(df$subject_id)

# Per-subject constants grouped by matrix_size
df %>%
  distinct(matrix_size, subject_id, total_trials, practice_trials, transition_matrix, conditional_entropies) %>%
  group_by(matrix_size) %>%
  summarize(
    n_subjects = n_distinct(subject_id),
    total_trials = unique(total_trials),
    practice_trials = unique(practice_trials),
    transition_matrix = list(unique(transition_matrix)),
    conditional_entropies = list(unique(conditional_entropies))
  )

# Unique position sequences per matrix_size
df %>%
  group_by(matrix_size, subject_id) %>%
  summarize(
    sequence = paste(position, collapse = ","),
    .groups = "drop"
  ) %>%
  group_by(matrix_size) %>%
  summarize(
    n_subjects = n_distinct(subject_id),
    n_unique_sequences = n_distinct(sequence)
  )

df <- df[, !(names(df) %in% c("transition_matrix", "conditional_entropies"))]

# Shows only 1 unique transition matrix and conditional entropies for conditions 6x6, 7x7 and 8x8 because of an error in data collection for those conditions after removing row by row log of the transition matrix and conditional entropies from the data files to save space. However, the code to shuffle those matrices and corresponding conditional entropies are still the same, so we believe participants are still each seeing a shuffled matrix and a different sequence in these conditions.
```
```{r}
#| label: Separate practice, main and questionnaire sections 
#| echo: false
#| include: false

df.practice <- df[df$phase=="practice", ]
df.questionnaire <- df[df$phase=="questionnaire", ]
df <- df[df$phase=="main", ]
```

### Data Exclusions
```{r}
#| label: exclude-subjects
#| echo: false
#| include: false
#| cache: true

# Exclude subjects that
# 1. Made too many mistakes (more than 15%)
# 2. Have too long response time (look at mean, median and spread)
# -> exclude if too slow--the subject's median_rt > 1000ms
# -> or if too noisy--more than 20% of their trials are 3*mad higher than the subject's median_rt

accuracy_excluded_subjects <- df %>%
  filter(matrix_size==8) %>%
  filter(!is.na(correct)) %>%
  group_by(subject_id) %>%
  summarize(error_rate = mean(!correct)) %>%
  filter(error_rate > 0.10) %>%
  pull(subject_id)

rt_excluded_subjects <- df %>%
  filter(
    (experiment_trial_type == "stimulus" | experiment_trial_type == "retry")
    & !is.na(rt)) %>% 
  group_by(subject_id) %>%
  summarize(
    median_rt = median(rt),
    outlier_prop = mean(rt > median(rt) + 3*mad(rt))
  ) %>%
  filter(median_rt > 1000 | outlier_prop > 0.2) %>%
  pull(subject_id)

df <- df %>%
  filter(!subject_id %in% accuracy_excluded_subjects & !subject_id %in% rt_excluded_subjects)
```
```{r}
#| label: Filter df down to only stimulus, non-retry trials
#| echo: false
#| include: false

df <- df %>%
  filter(experiment_trial_type=="stimulus" & !is.na(rt))
```
```{r}
#| label: Remove first trial of each block and rows with infinite log_rt (rt==0)
#| echo: false
#| include: false
#| cache: true

df <- df %>%
  filter(!is.infinite(log_rt)) %>%
  group_by(block, subject_id) %>%
  slice(2:n())
```
```{r}
#| label: Make a new column for log(rt), previous position entropy, prev_entropy_c, surprisal_c, is_repetition
#| echo: false
#| include: false
#| cache: true

df <- df %>%
  mutate(log_rt=log(rt)) %>%
  mutate(is_repetition=(position==lag(position,1))) %>%
  group_by(subject_id, block) %>%
  mutate(previous_entropy=lag(conditional_entropy, 1)) %>%
  ungroup() %>%
  group_by(matrix_size) %>%
  mutate(prev_entropy_c=previous_entropy-mean(previous_entropy, na.rm=TRUE)) %>%
  mutate(surprisal_c=surprisal-mean(surprisal, na.rm=TRUE)) %>%
  ungroup()
```
```{r}
#| label: Make subset of df with only correct trials
#| echo: false
#| include: false

df.correct <- df %>%
  filter(correct)
```

# Model - H0
```{r}
#| label: Learning effect lmer models
#| (filter for correct trials and group by matrix_size)
#| echo: true
#| include: true

df.m_h0 <- df %>%
  group_by(matrix_size)

m_h0_rt <-
  lmer(-log_rt ~ block + is_repetition + (block | subject_id) + (1 | position),
  data=df.m_h0,
  REML=FALSE,
  control=lmerControl(optimizer="bobyqa")) # can change

summary(m_h0_rt)

df.m_h0.correct <- df.correct %>%
  group_by(matrix_size)
  
m_h0_acc <-
  glmer(correct ~ block + is_repetition + (block | subject_id) + (1 | position),
  data=df.m_h0,
  family=binomial,
 control=glmerControl(optimizer="bobyqa")) # can change

summary(m_h0_acc)
```
## H1 & H2. Surprisal and previous position entropy effects

For H1 & H2, we analyze each block independently. First, we determine the effects structure by analyzing only the final block as follows: *(When the best-fit base model for surprisal and/or previous position entropy effects has been found, we can then examine learning trajectories by adding block as a continuous predictor)*

```{r}
#| label: Surprisal x previous position entropy lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

data.m_h1h2 <- df %>%
  filter(block==EXPERIMENT_CONFIG[["n_blocks"]]-1, !is.infinite(log_rt)) %>%
  group_by(matrix_size)

data.m_h1h2.correct <- df.correct %>%
  filter(block==EXPERIMENT_CONFIG[["n_blocks"]]-1) %>%
  group_by(matrix_size)

m_surprisal_x_entropy_rt <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

m_surprisal_x_entropy_acc <-
  glmer(correct ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       control=glmerControl(optimizer="bobyqa")) # can change

summary(m_surprisal_x_entropy_rt)
summary(m_surprisal_x_entropy_acc)
```
solve singular fit: 

+ remove random slope until isSingular() is False
+ compare models AIC, and our design mainly focus on surperisal and entropy, final random slope without singular fit exclude interaction and is_repetition


```{r}
m_surprisal_x_entropy_rt_simple <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c + prev_entropy_c | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

summary(m_surprisal_x_entropy_rt_simple)
isSingular(m_surprisal_x_entropy_rt_simple)

# AIC(m_surprisal_x_entropy_rt, m_surprisal_x_entropy_rt_simple)

fixef_comparison <- data.frame(
  Complex = fixef(m_surprisal_x_entropy_rt),
  Simple = fixef(m_surprisal_x_entropy_rt_simple),
  Diff_pct = 100 * abs(fixef(m_surprisal_x_entropy_rt) - fixef(m_surprisal_x_entropy_rt_simple)) / 
             abs(fixef(m_surprisal_x_entropy_rt_simple))
)

print(fixef_comparison)
```


```{r}
#| label: Surprisal x previous position entropy blmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: false
#| include: false

m_surprisal_x_entropy_rt_blm <- 
  blmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) + (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_x_entropy_acc_blm <-
  bglmer(correct ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) + (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_x_entropy_rt_blm)
summary(m_surprisal_x_entropy_acc_blm)

confint(m_surprisal_x_entropy_rt_blm, method="Wald") # if CI including 0 usually indicate not significant
```

```{r}
#| label: Surprisal x previous position entropy brm models
#| 
#| echo: false
#| include: false

m_surprisal_x_entropy_rt_brm <- brm(
  log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
    (surprisal_c * prev_entropy_c + is_repetition | subject_id) + (1 | position),
  data = data.m_h1h2)

summary(m_surprisal_x_entropy_rt_brm)
```

### Prune random slopes

base model: `log_rt ~ surprisal_c * prev_entropy_c + is_repetition + (surprisal_c + prev_entropy_c | subject_id) + (1 | position)`

result: we keep random effects: `(surprisal_c + prev_entropy_c | subject_id) + (1 | position)`


```{r}
#| label: Surprisal x previous position entropy lmer models
#| Prune: remove random slopes until AIC smallest
#| 
#| echo: true
#| include: true

m_surprisal_x_entropy_rt_rand_slope_1 <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c | subject_id) + (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

m_surprisal_x_entropy_rt_rand_slope_2 <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (prev_entropy_c | subject_id) + (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

m_surprisal_x_entropy_rt_rand_slope_3 <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c + prev_entropy_c || subject_id)+ (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

m_surprisal_x_entropy_rt_rand_slope_4 <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c + prev_entropy_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

AIC(m_surprisal_x_entropy_rt_simple, m_surprisal_x_entropy_rt_rand_slope_1, m_surprisal_x_entropy_rt_rand_slope_2, m_surprisal_x_entropy_rt_rand_slope_3, m_surprisal_x_entropy_rt_rand_slope_4)

# anova(m_surprisal_x_entropy_rt_simple, m_surprisal_x_entropy_rt_rand_slope_2)
# summary(m_surprisal_x_entropy_rt_rand_slope_1)
```

### Prune fixed effects

base model: `log_rt ~ surprisal_c * prev_entropy_c + is_repetition + [random effects left in last section]`

result: we have to keep the base model as the final model

```{r}
#| label: Surprisal x previous position entropy lmer models
#| Prune: remove fixed effects until AIC smallest
#| 
#| echo: true
#| include: true

# base model: m_surprisal_x_entropy_rt_rand_slope_1

m_surprisal_x_entropy_rt_fixed_effects_1 <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c +
         (surprisal_c + prev_entropy_c | subject_id) + (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

m_surprisal_x_entropy_rt_fixed_effects_2 <- 
  lmer(log_rt ~ surprisal_c + prev_entropy_c + is_repetition +
         (surprisal_c + prev_entropy_c | subject_id) + (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

AIC(m_surprisal_x_entropy_rt_simple, m_surprisal_x_entropy_rt_fixed_effects_1, m_surprisal_x_entropy_rt_fixed_effects_2)
```

### previous code

*We then prune the model as follows:*

If the model fails to converge, we will first try blmer/bglmer to see if that addresses convergence.

If not, we will see if any of these random slope structures converges for subject_id (still using blmer/bglmer):

```{r}
#| label: Surprisal x previous position entropy lmer models
#| Prune: position
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_x_entropy_prune_position_rt <- 
  blmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_x_entropy_prune_position_acc <-
  bglmer(correct ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_x_entropy_prune_position_rt)
summary(m_surprisal_x_entropy_prune_position_acc)
```


```{r}
#| label: Surprisal x previous position entropy blmer models
#| Prune: position, repetition
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_x_entropy_prune_position_repetition_rt <- 
  blmer(log_rt ~ surprisal_c * prev_entropy_c +
         (surprisal_c * prev_entropy_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_x_entropy_prune_position_repetition_acc <-
  bglmer(correct ~ surprisal_c * prev_entropy_c  +
         (surprisal_c * prev_entropy_c | subject_id),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_x_entropy_prune_position_repetition_rt)
summary(m_surprisal_x_entropy_prune_position_repetition_acc)
```

And then we will try treating surprisal and previous position entropy as individual fixed effects, instead of interacting:

```{r}
#| label: Surprisal + previous position entropy lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_entropy_rt <-
  blmer(log_rt ~ surprisal_c + prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_entropy_acc <- 
    bglmer(correct ~ surprisal_c + prev_entropy_c + is_repetition +
           (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
           (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_entropy_rt)
summary(m_surprisal_entropy_acc)
```

```{r}
#| label: Surprisal + previous position entropy effect lmer models
#| Prune: position
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_entropy_prune_position_rt <-
  blmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_entropy_prune_position_acc <- 
    bglmer(correct ~ surprisal_c + prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_entropy_prune_position_rt)
summary(m_surprisal_entropy_prune_position_acc)
```

```{r}
#| label: Surprisal + previous position entropy effect lmer models
#| Prune: position, repetition
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_entropy_prune_position_repetition_rt <-
  blmer(log_rt ~ surprisal_c * prev_entropy_c +
         (surprisal_c * prev_entropy_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_entropy_prune_position_repetition_acc <- 
    bglmer(correct ~ surprisal_c + prev_entropy_c  +
         (surprisal_c * prev_entropy_c | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_entropy_prune_position_repetition_rt)
summary(m_surprisal_entropy_prune_position_repetition_acc)
```

If that still doesn't work, we can further separate out surprisal_c and prev_entropy_c:

```{r}
#| label: Surprisal effect lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_rt <-
  blmer(log_rt ~ surprisal_c +
         (surprisal_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_acc <- 
    bglmer(correct ~ surprisal_c +
         (surprisal_c | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_rt)
summary(m_surprisal_acc)
```

```{r}
#| label: Previous entropy effect lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_entropy_rt <-
  blmer(log_rt ~ prev_entropy_c +
         (prev_entropy_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_prev_entropy_c_acc <- 
    bglmer(correct ~ prev_entropy_c +
         (prev_entropy_c | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_entropy_rt)
summary(m_entropy_acc)
```

And look at what is_repetition contributes:

```{r}
#| label: Repetition effect lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_repetition_rt <-
  blmer(-log_rt ~ is_repetition +
         (is_repetition | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_repetition_acc <- 
    bglmer(correct ~ is_repetition +
         (is_repetition | subject_id),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_repetition_rt)
summary(m_repetition_acc)
```

If none of those converge, we check to see if we made an error somewhere, because at least one of them should converge! The model with the most random slopes that converges is now our base model (for the moment).

Next, we test the significance of random slopes. We iteratively remove random slopes and check whether the result is worse (has a higher AIC). As soon as we can't remove any random slopes without increasing AIC, that is now the random effects structure we will use.

Next, we use model-comparison to determine fixed effects. Our base model is: `~ surprisal_c * prev_entropy_c + is_repetition + [random effects]`

First, we check for a repetition effect by comparing the base model with: `~ surprisal_c * prev_entropy_c + [random effects]`

If the base model has a lower AIC, the effect of repetition is significant. (This is very, very likely). If not, our new base model is the one without the is_repetition effect. We use this base model below.

We test whether there is an interaction by comparing the above with: `~ surprisal_c  + prev_entropy_c + is_repetition + [random effects]`

If the simpler model has a higher AIC, the interaction is significant and we stop. There is a surprisal effect (H1) and an entropy effect (H2).

If there is no interaction, we now compare the no-interaction model to models that lack one of the main effects of entropy or surprisal. So we compare against two models. If AIC for either of the new models is lower, then the corresponding fixed effect is not significant and can be eliminated.

If both entropy and surprisal are significant (eliminating either increases AIC), we are done and interpret H1 and H2. Otherwise, we try eliminating both of these fixed effects. If that increases AIC over a model with one fixed effect, then that fixed effect is definitely significant. (Hopefully this doesn't happen with both fixed effects, which would suggest that surprisal and entropy are so correlated that we can't distinguish them.)

## H3. Moderation from number of positions

filter(correct) %\>% group_by(matrix_size, block) %\>% model()

```{r}
#| label: Num positions effect lmer models
#| (filter for correct trials and group by matrix_size and block)
#| 
#| echo: true
#| include: true

m_matrix_size_rt <-
  blmer(-log_rt ~ matrix_size + is_repetition + (block | subject_id) + (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

# m_matrix_size_rt <-
#  lmer(-log_rt ~ matrix_size + is_repetition + (block | subject_id) + (1 | position),
#       data=data.m_h1h2,
#       REML=FALSE,
#       control=lmerControl(optimizer="bobyqa")) # can change

m_matrix_size_acc <- 
    bglmer(correct ~ matrix_size + is_repetition + (block | subject_id) + (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_matrix_size_rt)
summary(m_matrix_size_acc)
```

For each of the fixed effects used for H1-H2, recover the effect size estimate. Now, for each fixed effect: `effect_size ~ matrix_size*block`

Note that there are no random effects. We use `lm()` and get p-values in the normal way.

Models will be fit with ML for model comparison; final models refit with REML for inference.
