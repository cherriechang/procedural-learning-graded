---
title: "procedural-learning_preregistration"
author: "Cherrie Chang"
format: html
editor: visual
---

```{r}
library(lme4)
library(blme)
```

# Study Information

## Title

Procedural Learning Task Using Transition Matrices with Graded Entropy

## Authors

Cherrie Chang, Tianyi Li, and Joshua Hartshorne

## Description

Procedural learning is traditionally studied via serial reaction time tasks. In a typical serial reaction time task trial, a stimulus appears in one of four positions on screen, and the participant is tasked to press a corresponding key as quickly as possible. These trials are designed to follow a statistical pattern that determines what the current target position is based on the position(s) one or more steps back. Participants' ability to implicitly learn motor sequences is measured by how their response times and/or accuracy improve as they are exposed to more and more trials that follow the pattern.

In this study, we use a new probabilistic paradigm to study procedural learning, where instead of using a deterministic, finite sequence with a fixed level of noise, we generate our stimulus position sequence using transition matrices that assign a different level of entropy to each position, such that it is harder to predict what the next position given the current position for some positions than others. We are interested in how this affects implicit procedural learning of the statistical patterns. We also vary between subjects the total number of positions to see how this modulates procedural learning.

## Hypotheses

> **H0. Procedural learning effect.** We predict participants to show implicit learning of the statistical structure across blocks, evidenced by a) decrease in response time across blocks and b) increase in accuracy across blocks.

> **H1a. Surprisal effect.** In each run of the experiment positions differ in how many next-positions are possible and how evenly distributed the transition probabilities are among them, which also means different next-position/bigram combinations for a given first position yield different levels of predictability and thus surprisal to the participant. We predict that participants will respond faster and more accurately on bigram transitions that are higher probability.

> **H1b. Previous positional entropy effect.** In each run of the experiment, positions differ in their transition entropy---how many next-positions are possible and how evenly distributed the transition probabilities are among them. We predict that this influences how easily participants can predict the next position based on a given position, so participants will respond a) faster and b) more accurately on trials where the previous trial has lower entropy.

> **H1c. Interaction effect between surprisal and previous positional entropy.** The effect of surprisal on rt will be modulated by entropy, such that surprisal effects are stronger at low-entropy (predictable) positions and weaker at high-entropy (uncertain) positions. This is because entropy determines prediction confidence. In low-entropy contexts, participants form strong, specific expectations, making prediction errors (high surprisal) more costly. In high-entropy contexts, predictions are weaker and more distributed, attenuating surprisal effects.

> **H2a. Prediction error reflecting surprisal effect.** We predict that as participants implicitly learn the underlying statistical distribution, the mistakes they make reflect this learning, i.e. across blocks, the mistakes they make will be a) increasingly on low probability bigram transitions and b) the false transitions they make will increasingly be higher probability bigram transitions...

> **H2b. Prediction error reflecting entropy effect.** ...and they will also be increasingly limited to higher previous positional entropy trials.

*(whether it is worth it to run H2a and H2b is based on whether there are enough error trials)*

> **H3. Moderation from number of positions.** We predict that the more positions there are to handle, the more difficult it is to learn the underlying statistical distribution, so participants with a larger number of positions will have a gentler learning slope, higher mean/median RT and also show worse performances/take more blocks to reach the same level of performance (in terms of both RT and accuracy) on the effects predicted in H1 (and H2).

*Note: Matrix size is confounded with entropy range, so effects may reflect entropy distribution rather than number of positions per se.*

# Design Plan

## Study type

Experiment

## Blinding

No blinding is involved in this study

## Study design

**Matrix size (between-subjects).** This study uses a between-subjects manipulation of the number of positions to handle, which is the same thing as the size of the transition matrix used to generate the position sequence. Participants are randomly assigned to one of five matrix size conditions (4x4, 5x5, 6x6, 7x7, 8x8). The transition matrix describes the number of positions on screen to respond to, and the probabilistic structure the transitions between positions follow. Because larger matrix sizes/sequences with more positions require more trials for the participant to observe the same number of trials on average for each position compared to smaller matrix sizes, we also vary the total number of trials across conditions, setting it as 20 blocks \* 10 \* matrix_length for each condition. We also vary the total number of practice trials similarly, with each condition getting 2 \* matrix_length number of practice trials.

## Randomization

Within each matrix size condition, all participants experience the same underlying entropy gradient across positions, but the mapping between screen positions and entropy levels is randomized through a double permutation procedure that shuffles the original matrix (of that size) using the same random permutation. This preserves the statistical properties of the original matrix － transition probabilities and entropy values － but decouples entropy levels from specific screen positions. The position sequence for each participant is subsequently generated using their individual, shuffled transition matrix, such that the sequence each participant experiences is different both within and between matrix size conditions.

# Sampling Plan

## Existing data

Registration prior to creation of data

## Data collection procedures

Participants will be recruited to complete a web-based experiment using Prolific, an online crowd-sourcing platform for behavioral research. Recruitment will be limited to participants with an approval rating of 95% or higher, and have completed at least 20 previous studies. In addition, participants must complete the experiment using a laptop or desktop computer (i.e., not their mobile phone).

Participants will be paid \$5 for completion, which works out to an estimated \$12 / hour for completion of the study for the slowest participants (we expect it to take at most 25 minutes to complete).

## Sample size

We will recruit 50 participants for each between-subjects condition of the study, for a total of N=250 participants across the five conditions.

## Sample size rationale

Due to the difficulty of performing a power analysis on our pilot size of 10 (2 for each condition) and this paradigm being new, we roughly estimate our sample size based on procedural learning literature that use related paradigms (ASRT/SRTT).

# Variables

## Manipulated variables

**Size of matrix.** We manipulated the size of the transition matrices used to generate the position sequence for each participant, which is equivalent to length of matrix and also the number of total positions. The five levels of this continuous variable are: 4, 5, 6, 7, 8.

**Transitional structure of matrix.** For each matrix size, we created an "original matrix" that had the following properties: 1) doubly stochastic － all row and column sums sum to 1 (±0.0001); 2) approach a uniform stationary distribution after 7 timesteps (stationary visitation for each state are within 0.001 of each other); 3) had a graded entropy structure across states (positions) － each matrix had an positional entropy range of 0.06-0.181 bits for lowest and 1.695-2.838 bits for highest, and within a matrix each position increases in entropy from the previous by 0.272-0.601 bits; and 4) avoided self loops as much as possible － all of the matrices had 0s in the diagonal up until the last two positions, and for those non-zero self loop transitions the probability does not exceed more than 1/matrix_length. These original matrices are then shuffled through a double permutation procedure to produce a new matrix for each participant while preserving the mentioned matrix properties.

## Measured variables (required)

**Response time.** For each trial we record the latency, in milliseconds, from the onset of the stimulus to the response key-press. This data will be used in confirmatory analyses.

**Response (for accuracy).** On each trial, participants press a key on their keyboard to respond to the position in which the stimulus appears. We record what key the participant presses, including keys that are outside the set of keys that the positions correspond to, and record whether the key-press was correct.

**Post-experiment questionnaire (for measuring explicit learning).** We include 5 questions at the end of the experiment to measure participants' explicit awareness of the underlying statistical patterns:

> **Q1. Did you notice anything special about the task?** Choices: "Yes", "No" If participants responded "Yes", they were prompted with a follow-up question with free text response: **Q1b. What did you notice?**

> **Q2. Did you notice any regularity in where the mole appeared?** Choices: "Yes", "No" If participants responded "Yes", they were prompted with a follow-up question with free text response: **Q2b. Can you describe the regularity?**

> **Q3. Did you use any strategy to help you respond faster?** Choices: "Yes", "No" If participants responded "Yes", they were prompted with a follow-up question with free text response: **Q3b. Can you describe the strategy?**

> **Q4. There WAS a regularity in the sequence. What do you think it was?** (Free text response) **Q4b. How confident are you in your answer?** Response is given on a Likert scale with 5 levels, with level 1 labelled as "Not at all" and level 5 labelled as "Very confident"

*Derived variables:* Surprisal = -log₂(P(current_position \| previous_position)) Entropy = -Σ P(next\|current) × log₂(P(next\|current)) Both centered across participants within each matrix_size condition

# Analysis Plan

## Statistical models (required)

*Our primary confirmatory hypothesis is H1a (surprisal effect on RT). All other hypotheses are secondary/exploratory and we interpret their p-values accordingly.* **H0. Learning effect.**

```{r}
#| label: Learning effect lmer models
#| (filter for correct trials and group by matrix_size)
#| 
#| echo: true
#| include: true

data.m_h0 <- df.correct %>%
  group_by(matrix_size)

m_h0_rt <-
  lmer(-log_rt ~ block + is_repetition + (block | subject_id) + (1 | position),
       data=data.m_h0,
       REML=FALSE,
       control=lmerControl(optimizer="bobyqa")) # can change

m_h0_acc <-
  glmer(correct ~ block + is_repetition + (block | subject_id) + (1 | position),
       data=data.m_h0,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_h0_rt)
summary(m_h0_acc)
```

**H1 & H2. Surprisal and previous position entropy effects.** For H1 & H2, we analyze each block independently. First, we determine the effects structure by analyzing only the final block as follows: *(When the best-fit base model for surprisal and/or previous position entropy effects has been found, we can then examine learning trajectories by adding block as a continuous predictor)*

```{r}
#| label: Surprisal x previous position entropy lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

data.m_h1h2 <- data.correct %>%
  filter(block==EXPERIMENT_CONFIG[["n_blocks"]]-1) %>%
  group_by(matrix_size)

m_surprisal_x_entropy_rt <- 
  lmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_x_entropy_acc <-
  glmer(correct ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_x_entropy_rt)
summary(m_surprisal_x_entropy_acc)
```

We then prune the model as follows:

If the model fails to converge, we will first try blmer/bglmer to see if that addresses convergence.

If not, we will see if any of these random slope structures converges for subject_id (still using blmer/bglmer):

```{r}
#| label: Surprisal x previous position entropy lmer models
#| Prune: position
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_x_entropy_prune_position_rt <- 
  blmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_x_entropy_prune_position_acc <-
  bglmer(correct ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_x_entropy_prune_position_rt)
summary(m_surprisal_x_entropy_prune_position_acc)
```

```{r}
#| label: Surprisal x previous position entropy lmer models
#| Prune: position, repetition
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_x_entropy_prune_position_repetition_rt <- 
  blmer(log_rt ~ surprisal_c * prev_entropy_c +
         (surprisal_c * prev_entropy_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_x_entropy_prune_position_repetition_acc <-
  bglmer(correct ~ surprisal_c * prev_entropy_c  +
         (surprisal_c * prev_entropy_c | subject_id),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_x_entropy_prune_position_repetition_rt)
summary(m_surprisal_x_entropy_prune_position_repetition_acc)
```

And then we will try treating surprisal and previous position entropy as individual fixed effects, instead of interacting:

```{r}
#| label: Surprisal + previous position entropy lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_entropy_rt <-
  blmer(log_rt ~ surprisal_c + prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_entropy_acc <- 
    bglmer(correct ~ surprisal_c + prev_entropy_c + is_repetition +
           (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
           (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_entropy_rt)
summary(m_surprisal_entropy_acc)
```

```{r}
#| label: Surprisal + previous position entropy effect lmer models
#| Prune: position
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_entropy_prune_position_rt <-
  blmer(log_rt ~ surprisal_c * prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_entropy_prune_position_acc <- 
    bglmer(correct ~ surprisal_c + prev_entropy_c + is_repetition +
         (surprisal_c * prev_entropy_c + is_repetition | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_entropy_prune_position_rt)
summary(m_surprisal_entropy_prune_position_acc)
```

```{r}
#| label: Surprisal + previous position entropy effect lmer models
#| Prune: position, repetition
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_entropy_prune_position_repetition_rt <-
  blmer(log_rt ~ surprisal_c * prev_entropy_c +
         (surprisal_c * prev_entropy_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_entropy_prune_position_repetition_acc <- 
    bglmer(correct ~ surprisal_c + prev_entropy_c  +
         (surprisal_c * prev_entropy_c | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_entropy_prune_position_repetition_rt)
summary(m_surprisal_entropy_prune_position_repetition_acc)
```

If that still doesn't work, we can further separate out surprisal_c and prev_entropy_c:

```{r}
#| label: Surprisal effect lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_surprisal_rt <-
  blmer(log_rt ~ surprisal_c +
         (surprisal_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_surprisal_acc <- 
    bglmer(correct ~ surprisal_c +
         (surprisal_c | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_surprisal_rt)
summary(m_surprisal_acc)
```

```{r}
#| label: Previous entropy effect lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_entropy_rt <-
  blmer(log_rt ~ prev_entropy_c +
         (prev_entropy_c | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_prev_entropy_c_acc <- 
    bglmer(correct ~ prev_entropy_c +
         (prev_entropy_c | subject_id) +
         (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_entropy_rt)
summary(m_entropy_acc)
```

And look at what is_repetition contributes:

```{r}
#| label: Repetition effect lmer models
#| (filter for correct trials + final block and group by matrix_size)
#| 
#| echo: true
#| include: true

m_repetition_rt <-
  blmer(-log_rt ~ is_repetition +
         (is_repetition | subject_id),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_repetition_acc <- 
    bglmer(correct ~ is_repetition +
         (is_repetition | subject_id),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_repetition_rt)
summary(m_repetition_acc)
```

If none of those converge, we check to see if we made an error somewhere, because at least one of them should converge! The model with the most random slopes that converges is now our base model (for the moment).

Next, we test the significance of random slopes. We iteratively remove random slopes and check whether the result is worse (has a higher AIC). As soon as we can't remove any random slopes without increasing AIC, that is now the random effects structure we will use.

Next, we use model-comparison to determine fixed effects. Our base model is: `~ surprisal_c * prev_entropy_c + is_repetition + [random effects]`

First, we check for a repetition effect by comparing the base model with: `~ surprisal_c * prev_entropy_c + [random effects]`

If the base model has a lower AIC, the effect of repetition is significant. (This is very, very likely). If not, our new base model is the one without the is_repetition effect. We use this base model below.

We test whether there is an interaction by comparing the above with: `~ surprisal_c  + prev_entropy_c + is_repetition + [random effects]`

If the simpler model has a higher AIC, the interaction is significant and we stop. There is a surprisal effect (H1) and an entropy effect (H2).

If there is no interaction, we now compare the no-interaction model to models that lack one of the main effects of entropy or surprisal. So we compare against two models. If AIC for either of the new models is lower, then the corresponding fixed effect is not significant and can be eliminated.

If both entropy and surprisal are significant (eliminating either increases AIC), we are done and interpret H1 and H2. Otherwise, we try eliminating both of these fixed effects. If that increases AIC over a model with one fixed effect, then that fixed effect is definitely significant. (Hopefully this doesn't happen with both fixed effects, which would suggest that surprisal and entropy are so correlated that we can't distinguish them.)

**H3. Moderation from number of positions.** filter(correct) %\>% group_by(matrix_size, block) %\>% model()

```{r}
#| label: Num positions effect lmer models
#| (filter for correct trials and group by matrix_size and block)
#| 
#| echo: true
#| include: true

m_matrix_size_rt <-
  blmer(-log_rt ~ matrix_size + is_repetition + (block | subject_id) + (1 | position),
       data=data.m_h1h2,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

m_matrix_size_acc <- 
    bglmer(correct ~ matrix_size + is_repetition + (block | subject_id) + (1 | position),
       data=data.m_h1h2,
       family=binomial,
       REML=FALSE,
       control=lmerControl(optimizer="nloptwrap")) # can change

summary(m_matrix_size_rt)
summary(m_matrix_size_acc)
```

For each of the fixed effects used for H1-H2, recover the effect size estimate. Now, for each fixed effect: `effect_size ~ matrix_size*block`

Note that there are no random effects. We use `lm()` and get p-values in the normal way.

Models will be fit with ML for model comparison; final models refit with REML for inference.

## Transformations (optional)

As described above, RT is log-transformed on a per-trial basis.

## Inference criteria (optional)

See above

## Data exclusion (optional)

Trial-level exclusions: 1. rt \> 1000 ms 2. rt \> participant's median + 3\*mad (outlier) 3. first trial of blocks: we hypothesize that breaks between blocks may affect the participant's cognitive engagement with the first trial after a break, in terms of implicitly or explicitly treating it as a continuation of a sequence and/or following from the previous block's last trial and determined by the same underlying statistical structure.

Participant-level exclusions: 1. Outlier rate \> 30% 2. Accuracy \< 90% 3. Self-reported technical issues

The above may be too lenient or too stringent. We will use histograms to determine and adjust criteria if needed.

## Exploratory analysis (optional)

We will look at participants who reported detecting a pattern in the questionnaire, and determine whether the detected pattern is true of the underlying statistical structure. If both are true, this would indicate possible explicit learning from the participant, which may confound with the procedural learning effect we are measuring. We will then look at how many participants correctly detect patterns explicitly for each matrix size, and whether they actually perform better in the final blocks compared to participants who did not report detecting patterns, or detected false patterns.

# Other

## Other (Optional)
